---
title: Convex Regression
---

```{julia}
using ProximalDistanceAlgorithms
using Random, LinearAlgebra, Plots, DataFrames, CSV
gr(linewidth = 3, grid = false)
```

### Example 1: Euclidean norm

```{julia}
φ(x) = dot(x, x)    # function to estimate
d = 1               # number of covariates
n = 50              # number of samples
σ = 0.2             # standard deviation of perturbations

# create a problem instance
Random.seed!(1776)
y, y_truth, X = cvxreg_example(φ, d, n, σ)

# visualize 1D signal
figure = plot(xlabel = "x", ylabel = "phi(x)", title = "1D convex regression")
plot!(X', y_truth, label = "truth")
scatter!(X', y, label = "observed")

savefig(figure,
    joinpath("figures",
        "cvxreg_example_d=1_n=50_sigma=0.2.pdf"))

figure
```

```{julia}
y_scaled, X_scaled = mazumder_standardization(y, X)

figure = plot(xlabel = "x", ylabel = "phi(x)", title = "after standardization")
plot!(X_scaled', y_truth / norm(y_truth), label = "truth")
scatter!(X_scaled', y_scaled, label = "observed")

savefig(figure,
    joinpath("figures",
        "cvxreg_example_standardize_d=1_n=50_sigma=0.2.pdf"))

figure
```

##### Proximal Distance + Steepest Descent

```{julia}
struct SDLogger{vecT}
    γ::vecT     # step size
    g::vecT     # norm of gradient
    loss::vecT
    objective::vecT
    penalty::vecT
    iteration::Vector{Int}
end

function SDLogger(hint::Integer)
    γ = sizehint!(Float64[], hint)
    g = sizehint!(Float64[], hint)
    loss = sizehint!(Float64[], hint)
    objective = sizehint!(Float64[], hint)
    penalty = sizehint!(Float64[], hint)
    iteration = sizehint!(Int[], hint)

    return SDLogger(γ, g, loss, objective, penalty, iteration)
end

function (logger::SDLogger)(data, iteration)
    # log history every 50 iterations
    if iteration % 50 == 0
        # retrieve info stored in data
        γ, g, loss, objective, penalty = data

        # update fields
        push!(logger.γ, γ)
        push!(logger.g, g)
        push!(logger.loss, loss)
        push!(logger.objective, objective)
        push!(logger.penalty, penalty)
        push!(logger.iteration, iteration)
    end

    return nothing
end

function convergence_history(logger::SDLogger)
    figure = plot(title  = "convergence history",
        xlabel = "iteration",
        xscale = :log10,
        yscale = :log10,
        legend = :bottom)

    plot!(logger.iteration, logger.loss, label = "loss")
    plot!(logger.iteration, logger.objective, label = "objective")
    plot!(logger.iteration, logger.penalty, label = "penalty")
    plot!(logger.iteration, logger.g, label = "gradient")

    return figure
end
```

```{julia}
slow_schedule(ρ, iteration) = iteration % 500 == 0 ? ρ*1.1 : ρ
fast_schedule(ρ, iteration) = iteration % 50 == 0 ? ρ*1.1 : ρ
```

```{julia}
logger = SDLogger(5000 ÷ 50)

θ, ξ = @time cvxreg_fit(SteepestDescent(), y_scaled, X_scaled,
    maxiters = 10^4,
    ρ_init = 1.0,
    penalty = slow_schedule,
    history = logger)

figure = plot(xlabel = "x", ylabel = "phi(x)", title = "original scale")
plot!(X_scaled', y_truth, label = "truth")
scatter!(X_scaled', y_scaled*norm(y), label = "observed")
scatter!(X_scaled', θ*norm(y), label = "fit")

savefig(figure,
    joinpath("figures",
        "cvxreg_example_fit__d=1_n=50_sigma=0.2.pdf"))

figure
```

```{julia}
figure = convergence_history(logger)

savefig(figure,
    joinpath("figures",
        "cvxreg_example_history_d=1_n=50_sigma=0.2.pdf"))

figure
```

```{julia}
# store convergence history for each instance
logs = Dict{Tuple{Int,Int},typeof(logger)}()

# benchmark parameters
σ = 0.2
maxiters = 5*10^3
Random.seed!(1776)

# fields for DataFrame summary
ncovariates = Int[]
nsamples    = Int[]
loss        = Float64[]
objective   = Float64[]
penalty     = Float64[]
gradient    = Float64[]
cpu_time    = Float64[]

for n in (50, 100, 200, 500), d in (1, 2, 10)
    # simulate data
    y, y_truth, X = cvxreg_example(φ, d, n, σ)

    # standardize according to mazumder
    y_scaled, X_scaled = mazumder_standardization(y, X)

    # initialize logger for the problem
    history = SDLogger(maxiters ÷ 50)

    # solve the problem and record CPU time
    tt = @elapsed cvxreg_fit(SteepestDescent(), y_scaled, X_scaled,
        maxiters = maxiters,
        ρ_init   = 1.0,
        penalty  = slow_schedule,
        history  = history)

    # save the log and record entries for summary
    logs[(d,n)] = history

    push!(ncovariates, d)
    push!(nsamples, n)
    push!(loss, history.loss[end])
    push!(objective, history.objective[end])
    push!(penalty, history.penalty[end])
    push!(gradient, history.g[end])
    push!(cpu_time, tt)
end

# table summary
cvxreg_df = DataFrame(
    d = ncovariates,
    n = nsamples,
    cpu_time = cpu_time,
    loss = loss,
    objective = objective,
    penalty = penalty,
    gradient = gradient,
)

# save table to disk
CSV.write(joinpath("tables", "cvxreg_benchmarks.dat"), cvxreg_df)

# save convergence histories to disk
for ((d,n), history) in logs
    figure = convergence_history(history)
    savefig(figure,
        joinpath("figures",
            "cvxreg_d=$(d)_n=$(n)_sigma=$(σ).pdf"))
end

cvxreg_df
```

```{julia}
using Convex, Gurobi#, MosekTools

# benchmark parameters
σ = 0.2
maxiters = 5*10^3
Random.seed!(1776)

for n in (50, 100, 200), d in (1, 2, 10)
    # simulate data
    y, y_truth, X = cvxreg_example(φ, d, n, σ)

    # standardize according to mazumder
    y_scaled, X_scaled = mazumder_standardization(y, X)

    # solve with Gurobi
    _, _, problem = cvxreg_fit(BlackBox(), y_scaled, X_scaled)
    open(joinpath("tables", "cvxreg_gurobi_d=$(d)_n=$(n).dat"), "w") do file
        run() = solve!(problem, GurobiSolver())
        redirect_stdout(run, file)
    end

    # solve with Mosek
    # _, _, problem = cvxreg_fit(BlackBox(), y_scaled, X_scaled)
    # open(joinpath("tables", "cvxreg_mosek_d=$(d)_n=$(n).dat"), "w") do file
    #     run() = solve!(problem, Mosek.Optimizer())
    #     redirect_stdout(run, file)
    # end
end
```
