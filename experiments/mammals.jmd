---
title: Mammal Dentition
options:
    dpi: 155
---

```{julia; echo = false; results = "hidden"}
using ProximalDistanceAlgorithms
using CSV, DataFrames, Plots
using LinearAlgebra, MultivariateStats

import ProximalDistanceAlgorithms:
   __find_large_blocks!,
   adjacency_to_neighborhood,
   connect

Plots.reset_defaults()
gr(format = :png)

function count_clusters(Iv, Jv)
   d, n = size(X)

   A = ones(Int, n, n)
   for (i,j) in zip(Iv, Jv)
      A[i,j] = 0
      A[j,i] = 0
   end

   nbhood, _ = adjacency_to_neighborhood(A)
   component, components = connect(nbhood)

   return A, component, components
end

function count_clusters(W, X, k)
   # Iv, Jv, _ = find_large_blocks(W, X, k)
   # return count_clusters(Iv, Jv)
   d, n = size(X)

   A = zeros(Bool, n, n)
   for j in 1:n, i in j+1:n
      d_ij = distance(W, X, i, j)
      d_ij = round(d_ij^2, digits = 4)
      if d_ij^2 == 0
         A[i,j] = 1
         A[j,i] = 1
      end
   end

   nbhood, _ = adjacency_to_neighborhood(A)
   component, components = connect(nbhood)

   return A, component, components
end
```

This is an application of convex clustering to mammal dentition.
The convex clustering problem is solved using proximal distance algorithm that operates by steepest descent.

**Notation**

* $\boldsymbol{X}$: data with $d$ features and $n$ samples.
* $\boldsymbol{U}$: $d \times n$ matrix containing assigned centers.
* $\boldsymbol{D}$: fusion matrix
* $S_{K}$: structured sparsity set with $K$ non-zero blocks
* $K$: a *discrete* parameter that tunes the number of clusters.

*Unless otherwise noted, the terms below refer to the following quantities*

* loss: $\frac{1}{2} ||\boldsymbol{U} - \boldsymbol{X}||^{2}_{F}$
* penalty: $\frac{1}{2} \mathrm{dist}(\boldsymbol{D} \mathrm{vec}(\boldsymbol{U}), S_{K})$
* objective: loss + $\rho$ penalty

## Data

```{julia}
# load data
mammals = CSV.read("data/mammals.dat")

# delete that extra column at the beginning
mammals = mammals[:, 2:end]

# form input matrix for our clustering algorithm
X = mammals[:, 2:end] |> Matrix{Float64} |> transpose |> Matrix

# change first column to species
rename!(mammals, :X => :species)

first(mammals, 5)
```

## Dimensionality reduction with principal components

```{julia}
# extract number of features and samples
d, n = size(X)

# find principal components and map down to 2 dimensions
M = fit(PCA, X, maxoutdim = 2)
Z = transform(M, X)

# create point data
xs = Z[1,:]
ys = Z[2,:]

# create annotations
annotations = [text("$(mammals.species[i])", :black, :bottom, 8) for i in 1:n]

# plot the data using principal components
scatter(xs, ys, legend = false, grid = false)
xlabel!("PCA 1")
ylabel!("PCA 2")
annotate!(xs, ys, annotations)
```

## Distances using different weights

```{julia}
# create weights matrices
W = gaussian_weights(X, phi = 0.5)

# 'zero-out' weights based on k nearest neighbors
W_knn = knn_weights(W, 5)

# distances w/ Gaussian kernel
A_gauss = [W[i,j]*norm(X[:,i] - X[:,j]) for j in 1:n, i in 1:n]
heatmap(A_gauss, xrotation = 45, xtick_direction = :out)
title!("Euclidean distance (Guassian)")
xticks!(1:27, mammals.species)
yticks!(1:27, mammals.species)
```

```{julia}
# distances after thresholding w/ k nearest neighbors
A_knn = [W_knn[i,j]*norm(X[:,i] - X[:,j]) for j in 1:n, i in 1:n]
heatmap(A_knn, xrotation = 45, xtick_direction = :out)
title!("Euclidean distance (PCA)")
xticks!(1:27, mammals.species)
yticks!(1:27, mammals.species)
```

## Convex clustering with proximal distance surrogate

```{julia}
# this function implements a penalty schedule which is used to update
# the coefficient rho
penalty_sched(ρ, iteration) = min(1e6, iteration % 50 == 0 ? ρ*10.0 : ρ)

# set algorithm parameters
maxiters = 10^3
penalty_func = penalty_sched
K = 10

# convergence history
history = SDLogger(maxiters, 1)

# run the algorithm with SteepestDescent
U = @time convex_clustering(SteepestDescent(), W, X,
        penalty  = penalty_func,
        maxiters = maxiters,
        K        = K,
        history  = history
    );
# @. history.loss = abs(history.loss) + eps()
# @. history.objective = abs(history.objective) + eps()
# plot(history, linetype = :path, lw = 3, legend = :left)
```

```{julia}
# form distance matrix and plot it
A = [norm(U[:,i] - U[:,j]) for j in 1:n, i in 1:n]
heatmap(A, xrotation = 45, xtick_direction = :out)
title!("Euclidean distance (convex clustering)")
xticks!(1:27, mammals.species)
yticks!(1:27, mammals.species)
```

We cannot identify cluster assignments based on `U` alone.
Instead, we apply the projection onto the sparsity set and pass to an adjacency matrix.
We say node `i` is adjacent to `j` if `U[:,i] - U[:,j] = 0`.
Cluster assignments are then determined based on the connected components of the underlying graph.

```{julia}
# get cluster assignments; returns:
# * Adj - adjacency matrix
# * component - cluster assignments
# * components - the total number of clusters
Adj, component, components = count_clusters(ones(n,n), U, K)

# show the adjacency matrix
heatmap(Adj, xrotation = 45, xtick_direction = :out)
xticks!(1:27, mammals.species)
yticks!(1:27, mammals.species)
```

```{julia}
# visualize clusters with colors
scatter(xs, ys, legend = false, grid = false, color = component, markersize = 8)
xlabel!("PCA 1")
ylabel!("PCA 2")
annotate!(xs, ys, annotations)
```

### Solution Path

The *solution path* traces the centroids of each cluster as we vary $K$.
This discrete tuning parameter may take values between $0$ ($1$ cluster) and $\binom{n}{2}$ ($n$ clusters).
The relationship between $K$ and the number of clusters is non-linear and need not be monotonic.

```{julia}
# set algorithm parameters
maxiters = 10^4
penalty_func = fast_schedule
ncomparison = binomial(n, 2)
K_stepsize = 1
K_path = 1:K_stepsize:ncomparison

history = SDLogger(length(K_path), maxiters)

solpath = @time convex_clustering_path(SteepestDescent(), W, X,
    maxiters = maxiters,
    penalty = penalty_func,
    history = history,
    K_path = K_path)
```

```{julia}
# check clusters based on U[:,i] - U[:,j] == 0
cluster_data = [count_clusters(ones(n,n), U, K) for (U, K) in zip(solpath, K_path)]

assignments = [item[2] for item in cluster_data]
ncluster    = [item[3] for item in cluster_data]

# for i in 2:length(K_path)
#    if ncluster[i] < ncluster[i-1]
#       ncluster[i] = ncluster[i-1]
#       assignments[i] = assignments[i-1]
#    end
# end

# show the relationship between K and clusters
plot(K_path, ncluster, linetype = :steppre, legend = false)
title!("w/ equal weights")
xlabel!("K (tuning parameter)")
ylabel!("number of clusters")
```

Example with $K = 330$:

```{julia}
K = 19

x = transform(M, solpath[K])
centroids = [tuple(x[:,i]...) for i in 1:n]

xs = x[1,:]
ys = x[2,:]

# scatter(xs, ys, marker = :star, markersize = 12, color = :blue)
scatter(xs, ys, markersize = 6,
    color = assignments[K],
    legend = false,
    grid = false)
title!("clusters = $(ncluster[K])")
xlabel!("PCA 1")
ylabel!("PCA 2")
annotate!(xs, ys, annotations)
```

We recover the centroids by taking group averages of the points `U[:,i]` for value of `K`.
This lets us trace coalescence of clusters.

```{julia}
using Statistics

# replace NaNs with original data
# for i in eachindex(solpath)
#     if any(isnan, U_list[i])
#         solpath[i] = copy(X)
#     end
# end

# map centers to low dimensional representation
U_pca = map(U -> transform(M, U), solpath)

centerT = Tuple{Float64,Float64}    # type for centroids
# path = [[tuple(U_pca[k][:,i]...) for k in eachindex(K_path)] for i in 1:n]  # assignments for each sample
path = [centerT[] for _ in 1:n]

for (a, c, U) in zip(assignments, ncluster, U_pca)
    centroids = centerT[]
    for k in 1:c
        # retrieve assignments for cluster k
        idx = findall(isequal(k), a)

        # compute the center by averaging assignments within a group
        U_mean = mean(U[:,idx], dims = 2)

        # convert to tuple for plotting
        push!(centroids, tuple(U_mean...))
    end

    # add centers to solution path
    for i in 1:n
        push!(path[i], centroids[a[i]])
    end
end

fig = plot()
title!("solution path based on $(maxiters) iterations")
xlabel!("PCA 1")
ylabel!("PCA 2")

for p in solpath
    plot!(p, color = :black, linetype = :path)
end

scatter!(xs, ys, legend = false, grid = false, color = palette(:default)[1])
annotate!(xs, ys, annotations)
fig
```
