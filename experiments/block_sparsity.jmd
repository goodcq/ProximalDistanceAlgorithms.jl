---
title: Block Sparsity Projection
---

These notes implement an efficient algorithm for block sparsity projection.
First, we develop the general approach and then apply it to a problem involving
pairwise distances.

```{julia}
using BenchmarkTools, Distances
using DataStructures: heapify!, percolate_down!
```

We start with a $d \times n$ matrix $\boldsymbol{U}$ where each column $\boldsymbol{u}_{j}$ is an observation with $d$ features.
Our goal is to determine a sparse representation of the differences (arranged in blocks) $\boldsymbol{z} = \boldsymbol{D}~\mathrm{vec}(\boldsymbol{U})$ which effectively set $\boldsymbol{u}_{j} = \boldsymbol{u}_{i}$ for pairs $(i,j)$.
The distances $\Delta_{ij} = \|\boldsymbol{u}_{j} - \boldsymbol{u}_{i}\|$ are a useful proxy in solving this problem.
Concretely, our goal is to select the $\nu$-largest elements among the non-redundant elements of $\boldsymbol{\Delta}$.
Note that one is free to choose any metric $\|\cdot\|$.

There are a few computational issues to consider.
Sorting is unavoidable in computing a sparsity projeciton.
However, only a small subset of the sorting permutation is needed.
Let $L$ denote the number of non-trivial, non-redundant entries in $\boldsymbol{\Delta}$ (no diagonal elements and only the lower or upper triangular region).
If $\nu \le L/2$, then $P(\boldsymbol{z})$ is highly sparse.
On the other hand, if $\nu > L/2$ then $z - P(z)$ is highly sparse.
In the former case it is more efficient to directly select the $\nu$-largest elements in $\boldsymbol{\Delta}$, whereas the $(L-\nu)$-smallest elements are easier to compute in the latter case.
Partial sorting methods such as partial quicksort are well-suited to this problem.
Binary heaps are also efficient in partially sorting $\boldsymbol{\Delta}$.

Yet, even a size $\nu$ subset of the sorting permutation is excessive.
For example, in the case where determining the $\nu$-largest elements is optimal it is enough to determine two elements: the first and the $\nu$-th elements.
This observation is likely to be of little consequence to theory but it suggests a faster, in-place implementation of the projection operator.
That is, we view $P$ as an operator parameterized by two values, $\Delta_{\mathrm{lo}}$ and $\Delta_{\mathrm{hi}}$, and define
$$
P(\Delta_{ij};\Delta_{\mathrm{lo}},\Delta_{\mathrm{hi}}) =
\begin{cases}
    \Delta_{ij}, & \Delta_{\mathrm{lo}} \le \Delta_{ij} \le \Delta_{\mathrm{hi}} \\
    0, & \text{otherwise}
\end{cases}
$$
The cases are reversed if we choose to parameterize $P$ in terms of the smaller entries.
Thus, we compute the projection in two steps:

1. Given $\boldsymbol{\Delta}$, determine the projection $P(\cdot;\Delta_{\mathrm{lo}},\Delta_{\mathrm{hi}})$.

2. Given the projection operator $P$, compute $P(\Delta)$.

### Julia implementation for sparse projection

We implement this operator using a functor in Julia.

```{julia}
const MaxParam = Base.Order.Reverse
const MinParam = Base.Order.Forward
const MaxParamT = typeof(MaxParam)
const MinParamT = typeof(MinParam)

struct SparseProjection{order,T} <: Function
    lo::T
    hi::T
end

SparseProjection{order}(lo, hi) where {order} = SparseProjection{order,promote_type(typeof(lo), typeof(hi))}(lo, hi)
SparseProjection(order,lo,hi) = SparseProjection{typeof(order)}(lo,hi)

# parameterize by largest entries
(P::SparseProjection{MaxParamT})(x) = P.lo ≤ x ≤ P.hi ? x : zero(x)

# parameterize by smallest entries
(P::SparseProjection{MinParamT})(x) = P.lo ≤ x ≤ P.hi ? zero(x) : x
```

Now we define algorithms to compute the correct projection operator using partial sorting methods.

```{julia}
function partial_quicksort(xs, ord, K)
    sort!(xs, alg = PartialQuickSort(K), order = ord)
    lo, hi = extrema((xs[1], xs[K]))
    return SparseProjection(ord, lo, hi)
end

function swap!(h, i, j)
    h[i], h[j] = h[j], h[i]
    return nothing
end

function partial_heapsort(xs, ord, K)
    n = length(xs)
    heapify!(xs, ord)
    j = 1
    while j < K && j < n
        swap!(xs, 1, n-j+1)
        percolate_down!(xs, 1, xs[1], ord, n-j)
        j += 1
    end
    J = max(1, n-K)
    swap!(xs, 1, J)
    lo, hi = extrema((xs[J], xs[n]))
    return SparseProjection(ord, lo, hi)
end

extract_params_closure(ord, K, F) = xs -> F(xs, ord, K)
```

Simulate a distance matrix:

```{julia}
d, n = 3, 50
U = randn(d, n)                             # cluster data
Δ = pairwise(SqEuclidean(), U, dims = 2)    # distance matrix
x = [Δ[i,j] for i in 1:n for j in i+1:n]    # non-redundant representation
L = length(x)

Δ
```

##### Example: Projection 'max' parameterization.

If $\nu \ll L/2$, then we parameterize the projection by the $\nu$-largest values.

```{julia}
ν = 50              # sparsity level
ord = MaxParam      # optimal search direction

by_quicksort = extract_params_closure(ord, ν, partial_quicksort)
by_heapsort  = extract_params_closure(ord, ν, partial_heapsort)
```

##### Checking for correctness

```{julia}
P1 = by_quicksort(copy(x))
P2 = by_heapsort(copy(x))

P1.(x) == P2.(x)
```

##### Using the projection operator

```{julia}
P1.(Δ)
```

##### Benchmarking

```{julia}
@benchmark by_quicksort(ys) setup = (ys = copy($x))
```

```{julia}
@benchmark by_heapsort(ys) setup = (ys = copy($x))
```

##### Example: Projection 'min' parameterization.

If $\nu \gg L/2$, then we parameterize the projection by the $(L-\nu)$-largest values.

```{julia}
ν = L - 100     # sparsity level
ord = MinParam  # optimal search direction

by_quicksort = extract_params_closure(ord, L-ν, partial_quicksort)
by_heapsort  = extract_params_closure(ord, L-ν, partial_heapsort)
```

##### Checking for correctness

```{julia}
P1 = by_quicksort(copy(x))
P2 = by_heapsort(copy(x))

P1.(x) == P2.(x)
```

##### Using the projection operator

```{julia}
P1.(Δ)
```

##### Benchmarking

```{julia}
@benchmark by_quicksort(ys) setup = (ys = copy($x))
```

```{julia}
@benchmark by_heapsort(ys) setup = (ys = copy($x))
```

#### Appendix

```{julia}
using InteractiveUtils; versioninfo()
```

```{julia}
using Pkg; Pkg.status()
```
