\documentclass[12pt]{article}

\usepackage{amsbsy,amsthm,amsmath,amssymb}
%\usepackage{graphicx,natbib,booktabs}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{subfig}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{paralist}

\newtheorem{proposition}{Proposition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{definition}{Definition}[section]
\def\E{\mathop{\rm E\,\!}\nolimits}
\def\Var{\mathop{\rm Var}\nolimits}
\def\Cov{\mathop{\rm Cov}\nolimits}
\def\den{\mathop{\rm den}\nolimits}
\def\midd{\mathop{\,|\,}\nolimits}
\def\sgn{\mathop{\rm sgn}\nolimits}
\def\vec{\mathop{\rm vec}\nolimits}
\def\sinc{\mathop{\rm sinc}\nolimits}
\def\curl{\mathop{\rm curl}\nolimits}
\def\div{\mathop{\rm div}\nolimits}
\def\tr{\mathop{\rm tr}\nolimits}
\def\len{\mathop{\rm len}\nolimits}
\def\dist{\mathop{\rm dist}\nolimits}
\def\prox{\mathop{\rm prox}\nolimits}
\def\amp{\mathop{\:\:\,}\nolimits}
\def\Real{\mathop{\mathbb{R}}\nolimits}
\def\dom{\mathop{\bf dom}\nolimits}
\def\argmin{\mathop{\rm argmin}\nolimits}
\def\argmax{\mathop{\rm argmax}\nolimits}
\newcommand{\ba}{\boldsymbol{a}}
\newcommand{\bb}{\boldsymbol{b}}
\newcommand{\bc}{\boldsymbol{c}}
\newcommand{\bd}{\boldsymbol{d}}
\newcommand{\be}{\boldsymbol{e}}
\newcommand{\bff}{\boldsymbol{f}}
\newcommand{\bg}{\boldsymbol{g}}
\newcommand{\bh}{\boldsymbol{h}}
\newcommand{\bi}{\boldsymbol{i}}
\newcommand{\bj}{\boldsymbol{j}}
\newcommand{\bk}{\boldsymbol{k}}
\newcommand{\bl}{\boldsymbol{l}}
\newcommand{\bm}{\boldsymbol{m}}
\newcommand{\bn}{\boldsymbol{n}}
\newcommand{\bo}{\boldsymbol{o}}
\newcommand{\bp}{\boldsymbol{p}}
\newcommand{\bq}{\boldsymbol{q}}
\newcommand{\br}{\boldsymbol{r}}
\newcommand{\bs}{\boldsymbol{s}}
\newcommand{\bt}{\boldsymbol{t}}
\newcommand{\bu}{\boldsymbol{u}}
\newcommand{\bv}{\boldsymbol{v}}
\newcommand{\bw}{\boldsymbol{w}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bz}{\boldsymbol{z}}
\newcommand{\bA}{\boldsymbol{A}}
\newcommand{\bB}{\boldsymbol{B}}
\newcommand{\bC}{\boldsymbol{C}}
\newcommand{\bD}{\boldsymbol{D}}
\newcommand{\bE}{\boldsymbol{E}}
\newcommand{\bF}{\boldsymbol{F}}
\newcommand{\bG}{\boldsymbol{G}}
\newcommand{\bH}{\boldsymbol{H}}
\newcommand{\bI}{\boldsymbol{I}}
\newcommand{\bJ}{\boldsymbol{J}}
\newcommand{\bK}{\boldsymbol{K}}
\newcommand{\bL}{\boldsymbol{L}}
\newcommand{\bM}{\boldsymbol{M}}
\newcommand{\bN}{\boldsymbol{N}}
\newcommand{\bO}{\boldsymbol{O}}
\newcommand{\bP}{\boldsymbol{P}}
\newcommand{\bQ}{\boldsymbol{Q}}
\newcommand{\bR}{\boldsymbol{R}}
\newcommand{\bS}{\boldsymbol{S}}
\newcommand{\bT}{\boldsymbol{T}}
\newcommand{\bU}{\boldsymbol{U}}
\newcommand{\bV}{\boldsymbol{V}}
\newcommand{\bW}{\boldsymbol{W}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\bY}{\boldsymbol{Y}}
\newcommand{\bZ}{\boldsymbol{Z}}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\bepsilon}{\boldsymbol{\epsilon}}
\newcommand{\blambda}{\boldsymbol{\lambda}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bnu}{\boldsymbol{\nu}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bpi}{\boldsymbol{\pi}}
\newcommand{\bsigma}{\boldsymbol{\sigma}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bomega}{\boldsymbol{\omega}}
\newcommand{\bxi}{\boldsymbol{\xi}}
\newcommand{\bGamma}{\boldsymbol{\rho}}
\newcommand{\bDelta}{\boldsymbol{\Delta}}
\newcommand{\bTheta}{\boldsymbol{\Theta}}
\newcommand{\bLambda}{\boldsymbol{\Lambda}}
\newcommand{\bXi}{\boldsymbol{\Xi}}
\newcommand{\bPi}{\boldsymbol{\Pi}}
\newcommand{\bOmega}{\boldsymbol{\Omega}}
\newcommand{\bUpsilon}{\boldsymbol{\Upsilon}}
\newcommand{\bPhi}{\boldsymbol{\Phi}}
\newcommand{\bPsi}{\boldsymbol{\Psi}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
%% Matrix-matrix operations
%\newcommand{\Kron}{\otimes} %Kronecker
%\newcommand{\Khat}{\odot} %Khatri-Rao
\newcommand{\Hada}{\odot} %Hadamard
%\newcommand{\Divide}{\varoslash}

\title{Proximal Distance Thresholding}

\date{}
\author{}

\begin{document}
\maketitle

%\section*{\center SLeast Squares Version}

%As a convex relaxation of sparse least squares, consider the
%problem of minimizing the function $\frac{1}{2}\|\by-\bA (\bgamma_n \Hada \bs)\|^2$ subject to $\bs \in k S_p$, where $\bA$ is $n \times p$, $\Hada$ denotes the Hadamard product, and $S_p$ denotes the unit simplex in $\mathbb{R}^p$. Here $\bgamma_n = \bbeta_n-\nabla f(\bbeta_n)$ is the steepest descent update for $f(\bbeta)=\frac{1}{2}\|\by-\bA \bbeta\|^2$ at $\bbeta_n$. Given the optimal $\bs$, we produce $\bbeta_{n+1}$ by projecting $\bgamma_n \Hada \bs$ to sparsity. If we replace $\bA$ by $\bA \bD_n$, where $\bD_n$ is the diagonal matrix with $\bgamma_n$ along its diagonal, then our problem reduces to minmizing $\frac{1}{2}\|\by-\bA \bD_n \bs)\|^2$ subject to $\bs \in k S_p$. From the proximal distance
%perspective, we minimize 
%\begin{eqnarray*}
%\frac{1}{2}\|\by-\bA \bD_n \bs)\|^2 + \frac{\rho}{2}\dist(\bs, S_p)^2.
%\end{eqnarray*}
%This problem has the surrogate
%\begin{eqnarray*}
%\frac{1}{2}\|\by-\bA \bD_n \bs)\|^2 + \frac{\rho}{2}\|\bs-P_{S_p}(\bs_k)\|^2 & = &
%\left\|\begin{pmatrix}\bA\bD_n \\ \sqrt{\rho}\bI_p \end{pmatrix}\bs
%-\begin{pmatrix}\by \\ \sqrt{\rho}P_{S_p}(\bs_k) \end{pmatrix}\right\|^2.
%\end{eqnarray*}
%This notation suggests solving the problem by a few rounds of least squares.


As an approach to sparse least squares, define the loss $f(\bbeta)=\frac{1}{2}\|\by-\bA\bbeta\|^2$ and objective $f(\bbeta)+\frac{\rho}{2}\dist(\bbeta, S_k)^2$,
where $S_k$ is the sparsity set $\|\bbeta\|_0 \le k$. Gradient based minimization in the MM framework operates by  minimizing 
\begin{eqnarray*}
g(\gamma \mid \bbeta_n)& = & f(\bbeta_n + \gamma \bv_n)
+ \frac{\rho}{2}\|\bbeta_n + \gamma \bv_n- P_{S_k}(\bbeta_n)\|^2 \\
& \ge &  f(\bbeta_n + \gamma \bv_n) + \frac{\rho}{2}\dist(\bbeta_n + \gamma \bv_n, S_k)^2 ,
\end{eqnarray*}
where $\bv_n = -\nabla f(\bbeta_n)- \rho[\bbeta_n-P_{S_k}(\bbeta_n)]$
is the direction of steepest descent. The corresponding stationarity condition 
\begin{eqnarray*}
0 & = & -\bv_n^t\bA^t[\by-\bA (\bbeta_n + \gamma \bv_n)] 
+ \rho \bv_n^t[\bbeta_n + \gamma \bv_n- P_{S_k}(\bbeta_n)] 
\end{eqnarray*}
has solution
\begin{eqnarray*}
\gamma & = & \frac{\bv_n^t[\bA^t(\by-\bA \bbeta_n) -\rho \bbeta_n +\rho P_{S_k}(\bbeta_n)]}
{\|\bA \bv_n\|^2 + \rho \|\bv_n\|^2} \\
& = & \frac{\|\bv_n\|^2}{\|\bA \bv_n\|^2 + \rho \|\bv_n\|^2}. 
\end{eqnarray*}

If $f(\bbeta)$ is instead the more convex quadratic $\frac{1}{2}\bbeta^t\bC\bbeta+\bd^t\bbeta$, then $\nabla f(\bbeta) = \bC\beta+\bd$, and the
stationarity condition becomes
\begin{eqnarray*}
0 & = & \bv_n^t\nabla f(\bbeta_n+\eta \bv_n)
+ \rho \bv_n^t[\bbeta_n + \gamma \bv_n- P_{S_k}(\bbeta_n)] \\
& = & \bv_n^t[\bC(\bbeta_n+\eta \bv_n^t)+\bd]
+ \rho \bv_n^t[\bbeta_n + \gamma \bv_n- P_{S_k}(\bbeta_n)] \\
& = & -\bv_n^t\bv_n+ \eta (\bv_n^tC\bv_n+\rho \bv_n^t\bv_n). 
\end{eqnarray*}
The update $\bbeta_{n+1}=\bbeta_n + \gamma \bv_n$ now has
\begin{eqnarray*}
\gamma & = & \frac{\|\bv_n\|^2}{\bv_n^t \bC \bv_n + \rho \|\bv_n\|^2}. 
\end{eqnarray*}
This opens up the possibility of approximating $f(\bbeta)$ by
a quadratic involving an expected information matrix and hence
the whole problem area of sparse generalized linear models.
The virtue of the update is that it obeys the descent property 
for the proximal distance objective. Also no matrix inversion is
required. Finally, any closed set $S$ with a simple projection
operator can be substituted for $S_k$.

If we replace the constraint $\bbeta \in S$ by the fusion constraint
$\bD\bbeta \in S$ for some matrix $\bD$, then we should replace the objective by $f(\bbeta)+\frac{\rho}{2}\dist(\bD\bbeta,S)^2$. The appropriate gradient is now
\begin{eqnarray*}
\bv_n & = &  -\nabla f(\bbeta_n)- \rho\bD^t[\bD\bbeta_n-P_{S}
(\bD\bbeta_n)].
\end{eqnarray*}
If we further assume that $f(\bbeta)$ is the convex quadratic 
$\frac{1}{2}\bbeta^t\bC\bbeta+\bd^t\bbeta$, then then the optimal step length becomes 
\begin{eqnarray*}
\gamma & = & \frac{\|\bv_n\|^2}{\bv_n^t \bC \bv_n + \rho \|\bD\bv_n\|^2}. 
\end{eqnarray*}
One can show that the denominator here is always positive for $\bv_n \ne {\bf 0}$ provided $\rho$ is large enough and $\bv_n^t \bC \bv_n>0$
whenever $\bD\bv_n = {\bf 0}$ and $\bv_n \ne {\bf 0}$ \cite{debreu52}.
 
\begin{thebibliography}{99}
\bibitem{debreu52}
Debreu G (1952) Definite and semidefinite quadratic forms. {\it Econometrica}
20:295--300
\end{thebibliography}

\end{document}
