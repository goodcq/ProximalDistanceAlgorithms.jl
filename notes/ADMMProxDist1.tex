\documentclass[11pt]{article}
\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage{graphicx,url}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{example}{Example}[section]
%\newcommand{\qed}{\hfill\rule{2mm}{2mm}}
\newcommand{\svskip}{\vspace{1.75mm}}
\newcommand{\mvskip}{\vspace{.25in}}
\newcommand{\lvskip}{\vspace{.5in}}
\def\E{\mathop{\rm E\,\!}\nolimits}
\def\Var{\mathop{\rm Var}\nolimits}
\def\Cov{\mathop{\rm Cov}\nolimits}
\def\den{\mathop{\rm den}\nolimits}
\def\midd{\mathop{\,|\,}\nolimits}
\def\sgn{\mathop{\rm sgn}\nolimits}
\def\vec{\mathop{\rm vec}\nolimits}
\def\sinc{\mathop{\rm sinc}\nolimits}
\def\curl{\mathop{\rm curl}\nolimits}
\def\div{\mathop{\rm div}\nolimits}
\def\tr{\mathop{\rm tr}\nolimits}
\def\len{\mathop{\rm len}\nolimits}
\def\diag{\mathop{\rm diag}\nolimits}
\def\dist{\mathop{\rm dist}\nolimits}
\def\prox{\mathop{\rm prox}\nolimits}
\def\argmin{\mathop{\rm argmin}\nolimits}
\def\amp{\mathop{\;\:}\nolimits}
\newcommand{\ba}{\boldsymbol{a}}
\newcommand{\bb}{\boldsymbol{b}}
\newcommand{\bc}{\boldsymbol{c}}
\newcommand{\bd}{\boldsymbol{d}}
\newcommand{\be}{\boldsymbol{e}}
\newcommand{\bff}{\boldsymbol{f}}
\newcommand{\bg}{\boldsymbol{g}}
\newcommand{\bh}{\boldsymbol{h}}
\newcommand{\bi}{\boldsymbol{i}}
\newcommand{\bj}{\boldsymbol{j}}
\newcommand{\bk}{\boldsymbol{k}}
\newcommand{\bl}{\boldsymbol{l}}
\newcommand{\bm}{\boldsymbol{m}}
\newcommand{\bn}{\boldsymbol{n}}
\newcommand{\bo}{\boldsymbol{o}}
\newcommand{\bp}{\boldsymbol{p}}
\newcommand{\bq}{\boldsymbol{q}}
\newcommand{\br}{\boldsymbol{r}}
\newcommand{\bs}{\boldsymbol{s}}
\newcommand{\bt}{\boldsymbol{t}}
\newcommand{\bu}{\boldsymbol{u}}
\newcommand{\bv}{\boldsymbol{v}}
\newcommand{\bw}{\boldsymbol{w}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bz}{\boldsymbol{z}}
\newcommand{\bA}{\boldsymbol{A}}
\newcommand{\bB}{\boldsymbol{B}}
\newcommand{\bC}{\boldsymbol{C}}
\newcommand{\bD}{\boldsymbol{D}}
\newcommand{\bE}{\boldsymbol{E}}
\newcommand{\bF}{\boldsymbol{F}}
\newcommand{\bG}{\boldsymbol{G}}
\newcommand{\bH}{\boldsymbol{H}}
\newcommand{\bI}{\boldsymbol{I}}
\newcommand{\bJ}{\boldsymbol{J}}
\newcommand{\bK}{\boldsymbol{K}}
\newcommand{\bL}{\boldsymbol{L}}
\newcommand{\bM}{\boldsymbol{M}}
\newcommand{\bN}{\boldsymbol{N}}
\newcommand{\bO}{\boldsymbol{O}}
\newcommand{\bP}{\boldsymbol{P}}
\newcommand{\bQ}{\boldsymbol{Q}}
\newcommand{\bR}{\boldsymbol{R}}
\newcommand{\bS}{\boldsymbol{S}}
\newcommand{\bT}{\boldsymbol{T}}
\newcommand{\bU}{\boldsymbol{U}}
\newcommand{\bV}{\boldsymbol{V}}
\newcommand{\bW}{\boldsymbol{W}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\bY}{\boldsymbol{Y}}
\newcommand{\bZ}{\boldsymbol{Z}}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\bepsilon}{\boldsymbol{\epsilon}}
\newcommand{\blambda}{\boldsymbol{\lambda}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bnu}{\boldsymbol{\nu}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bpi}{\boldsymbol{\pi}}
\newcommand{\bsigma}{\boldsymbol{\sigma}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bomega}{\boldsymbol{\omega}}
\newcommand{\bxi}{\boldsymbol{\xi}}
\newcommand{\bGamma}{\boldsymbol{\Gamma}}
\newcommand{\bDelta}{\boldsymbol{\Delta}}
\newcommand{\bTheta}{\boldsymbol{\Theta}}
\newcommand{\bLambda}{\boldsymbol{\Lambda}}
\newcommand{\bXi}{\boldsymbol{\Xi}}
\newcommand{\bPi}{\boldsymbol{\Pi}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bUpsilon}{\boldsymbol{\Upsilon}}
\newcommand{\bPhi}{\boldsymbol{\Phi}}
\newcommand{\bPsi}{\boldsymbol{\Psi}}
\newcommand{\bOmega}{\boldsymbol{\Omega}}

\pagenumbering{gobble}

\title{Proximal Distance Minimization by ADMM}
\author{Names in Some Order
\\
\\
\\
Departments of Biostatistics, Computational Medicine,\\
Human Genetics, and Statistics \\
University of California \\
Los Angeles, CA 90095\\
Phone: 310-206-8076 \\
E-mail klange@ucla.edu \\
\\
\\
\\
\\
Research supported in part by USPHS grants GM53275 and HG006139.}

\begin{document}
\maketitle
%\newpage
%\begin{abstract}

\noindent  \\ \\
\lvskip
%\noindent {\bf Key Words:} 

%\end{abstract}

\newpage
\pagenumbering{arabic}

\baselineskip=20pt

\section*{\center Introduction}

The generic problem of minimizing a continuous function $f(\bx)$ over 
a closed set $S$ of $\mathbb{R}^p$ can be attacked by distance majorization. The classical penalty method seeks the solution of a penalized version $h_\rho(\bx)=f(\bx)+\rho q(\bx)$ of $f(\bx)$, where
the penalty $q(\bx)$ is nonnegative and 0 precisely when $\bx \in S$. If one follows the solution vector $\bx_\rho$ as $\rho$ tends to 
$\infty$, then in the limit one recovers the constrained solution \cite{beltrami1970algorithmic,courant1943variational}. The
function 
\begin{eqnarray*}
q(\bx) & = & \frac{1}{2} \dist(\bx,S)^2 
\amp = \amp \frac{1}{2}\min_{\by \in S} \|\bx-\by\|^2
\end{eqnarray*}
is one of the most fruitful penalties in this setting. Our previous research for solving this penalized minimimization problem has focused on an MM (majorization-minimization) algorithm based on distance majorization \cite{lange2016mm}. In distance majorization one constructs the surrogate function
\begin{eqnarray*}
g_\rho(\bx \mid \bx_n) & = & f(\bx)+\frac{\rho}{2}
\|\bx-P_{S}(\bx_n)\|^2 
\end{eqnarray*}
using the Euclidean projection $P_S(\bx_n)$ of the current iterate 
$\bx_n$ onto $S$. The minimum of the surrogate occurs at the proximal point
\begin{eqnarray*}
\bx_{n+1} & = & \prox_{\rho^{-1}f}[P_S(\bx_n)].
\end{eqnarray*}
According to the MM principle, this choice of $\bx_{n+1}$ decreases
$g_\rho(\bx \mid \bx_n)$ and hence the objective $h_\rho(\bx)$ as well.

We have named this iterative scheme the proximal distance algorithm \cite{keys19,lange2016mm}. It enjoys several virtues. 
First, it allows one to exploit the extensive body of results on proximal maps and projections. Second, it does not demand that the constraint set $S$ be convex. If $S$ is merely closed, then the map $P_C(\bx)$ may be multivalued, and one must choose a representative element from the projection $P_S(\bx_n)$. Third, the algorithm does not require the objective function $f(\bx)$ to be differentiable. Traditional penalty methods have been criticized for their numerical instability.  This hazard is mitigated in the proximal distance algorithm by its reliance on proximal maps, which usually are highly accurate.  The major defect of the proximal distance algorithm is slow convergence. This can be ameliorated by Nesterov acceleration \cite{nesterov04}.

This simple version of distance majorization can be generalized in various ways. For instance, it can be expanded to multiple constraint sets. In practice, at most two constraint sets usually suffice. We will take one of these to be a closed convex set $C$, which we view as the primary domain of $f(\bx)$. The base case $C=\mathbb{R}^p$ is allowed.
Another generalization is to replace the secondary constraint 
$\bx \in S$ by the constraint $\bD \bx \in S$, where $\bD$ is a
compatible matrix. Again, the base case $\bD=\bI$ is allowed. By
analogy with the fused lasso, we will call the matrix $\bD$ a fusion
matrix.

With the hope of achieving even faster convergence, the current paper investigates versions of ADMM (alternating direction method of multipliers) \cite{hong2016convergence} as possible substitutes for the 
proximal distance algorithm. For our purposes, this algorithm class is designed to minimize functions of the form $f(\bx)+g(\bD\bx)$ subject to $\bx \in C$, where $C$ is closed and convex. Splitting variables leads to the revised objective $f(\bx)+g(\by)$ subject to $\bx \in C$ and $\by=\bD\bx$. ADMM invokes the augmented Lagrangian
\begin{eqnarray*}
\mathcal{L}_\mu(\bx,\by, \blambda) & = & f(\bx)+ g(\by)
+ \mu \blambda^t(\bD\bx-\by)+\frac{\mu}{2} \|\bD\bx -\by\|^2 \\
& = & f(\bx)+ g(\by)+\frac{\mu}{2} \|\bD\bx -\by+\blambda \|^2 
-\frac{\mu}{2}\|\blambda\|^2
\end{eqnarray*}
with scaled Lagrange multiplier $\blambda$ and step length $\mu>0$. At iteration $n+1$ of ADMM one calculates successively 
\begin{eqnarray}
\bx_{n+1} 
& = & \argmin_{\bx \in C} \Big[f(\bx)+
\frac{\mu}{2} \|\bD\bx-\by_n +\blambda_n \|^2\Big] \label{admm1} \\
\by_{n+1} & = & \argmin_{\by} \Big[g(\by) +
\frac{\mu}{2} \|\bD\bx_{n+1}-\by +\blambda_n \|^2\Big] \label{admm2}\\
\blambda_{n+1} & = & \blambda_n+\mu(\bD\bx_{n+1}-\by_{n+1}). \label{admm3}
\end{eqnarray}
The $\bx_{n+1}$ update (\ref{admm1}) is given by the proximal map 
$\prox_{\mu^{-1}f}(\blambda_n-\by_n)$ when $C=\mathbb{R}^p$ and $\bD=\bI$. The $\blambda_{n+1}$ update (\ref{admm3}) amounts to steepest
ascent on the dual problem.

The $\by_{n+1}$ update (\ref{admm2}) is also given by a proximal map.
Fortunately, when $g(\by)=\frac{\rho}{2}\dist(\by,S)^2$ and $S$ is a closed convex set, the proximal map is determined by an explicit formula \cite{bauschke2017convex}. To derive this formula, note that  the proximal map $\by=\prox_{\alpha g}(\bz)$ satisfies the stationarity condition
\begin{eqnarray*}
{\bf 0} & = & \by-\bz+\alpha [\by-P_S(\by)] 
\end{eqnarray*}
for any $\bz$, including $\bz = \bD\bx_{n+1}+\blambda_{n}$, and 
any $\alpha$, including $\alpha = \rho/\mu$. Since the projection map $P_S(\by)$ has the constant value $P_S(\bz)$ on the line segment 
$[\bz,P_S(\bz)]$, the value 
\begin{eqnarray*}
\prox_{\alpha g}(\bz) & = & \frac{\alpha}{1+\alpha}P_S(\bz)+\frac{1}{1+\alpha}\bz
\end{eqnarray*}
satisfies the stationarity condition. Because the explicit update
for $\by_{n+1}$ decreases the Lagrangian even when $S$ is nonconvex,
we will employ it generally.   

When $f(\bx)$ is smooth, $C=\mathbb{R}^p$, but $\bD \neq \bI$, the
update $\bx_{n+1}$ is more problematic. Newton's method suggests
the update
\begin{eqnarray*}
\bx_{n+1} & = & \bx_n - [d^2f(\bx_n)+ \mu\bD^t\bD]^{-1}
[\nabla f(\bx_n)+\mu \bD^t (\bD\bx_n-\by_n+\blambda_n)]. 
\end{eqnarray*}
Although the contribution of $\mu\bD^t\bD$ to the Hessian improves
the chances for local convexity, it might be prudent to substitute
a positive definite approximation of $d^2f(\bx)$. In statistical applications, the expected information matrix is a natural
choice. In any event, it is crucial to retain as much curvature
information on $f(\bx)$ as possible.

Let us emphasize that passing to the ADMM setting has 
eliminated the need for distance majorization. Although distance
majorization is convenient, it is not a tight majorization. Thus,
one can hope to see gains in rates of convergence. Our numerical examples will bear out this expectation.

\section*{\center Convergence Analysis}

Recall that we are interested in minimizing $f(\bx)$ subject to
$\bx \in C$ and $\bD \bx \in S$. To find an approximate
solution, we minimize the penalized loss 
\begin{eqnarray*}
h_\rho(\bx) & = & f(\bx)+\frac{\rho}{2}\dist(\bD\bx, S)^2
\end{eqnarray*}
subject to $\bx \in C$ for $\rho$ large. Distance majorization gives the surrogate function
\begin{eqnarray*}
g_{\rho}(\bx \mid \bx_n) & = & 
f(\bx)+\frac{\rho}{2}\|\bD\bx-\bp_n\|^2,
\end{eqnarray*}
where $\bp_n  =  P_S(\bD\bx_n)$ denotes the projection of $\bD\bx_n$ onto $S$. The proximal distance algorithm proceeds iteratively by choosing
\begin{eqnarray*}
\bx_{n+1} & \in &  \underset{\bx \in C}\argmin \; g_\rho(\bx \mid \bx_n). 
\end{eqnarray*}
In the convex version of this framework, we have a first convergence result.	
	
\begin{proposition}
Supposes that $C$ and $S$ are closed and convex and that $f(\bx)$
is convex and differentiable. Then for any solution $\bz_\rho$ 
of our constrained minimization problem, the iterates of the
proximal distance algorithm satisfy  
\begin{eqnarray*}
0 \le & h_{\rho}(\bx_n) - h_\rho(\bz_\rho) \amp \le \amp
\frac{\rho}{2(n+1)} \|\bD\bz_\rho-\bD\bx_0 \|^2.
\end{eqnarray*}
Furthermore, the iterate values $h_\rho(\bx_n)$ systematically
decrease.
\end{proposition}
\begin{proof}
%{\bf Proof}: 
Systematic decrease of the iterate values $h_\rho(\bx_n)$
is a consequence of the MM principle. To prove the stated bound,
first observe that the function 
$g_\rho(\bx \mid \bx_n)-\frac{\rho}{2}\|\bD\bx\|^2$ is convex, being 
the sum of the convex function $f(\bx)$ and a linear function. Because
$\nabla g_\rho(\bx_{n+1} \mid \bx_n)^t(\bx-\bx_{n+1}) \ge \bf 0$ for any $\bx$ in $C$, it follows that
\begin{eqnarray*}
g_{\rho}(\bx \mid \bx_n)-\frac{\rho}{2}\|\bD\bx\|^2
& \ge & g_{\rho}(\bx_{n+1} \mid \bx_n)-\frac{\rho}{2} 
\|\bD\bx_{n+1}\|^2 \\
&  & -\rho \bx_{n+1}^t\bD^t\bD(\bx-\bx_{n+1}) ,
\end{eqnarray*}
or equivalently
\begin{eqnarray}
g_{\rho}(\bx \mid \bx_n) & \ge & g_{\rho}(\bx_{n+1} \mid \bx_n) 
+ \frac{\rho}{2}\|\bD(\bx-\bx_{n+1})\|^2. \label{error_bd1}
\end{eqnarray}
Now define the difference
\begin{eqnarray*}
d_{\rho}(\bx \mid \bx_n) & \!=\! & g_{\rho}(\bx \mid \bx_n)-h_{\rho}(\bx) 
\amp = \amp \frac{\rho}{2} \|\bx-P_S(\bx_n)\|^2 
-\frac{\rho}{2}\|\bx -P_S(\bx)\|^2.
\end{eqnarray*}
This function has gradient
\begin{eqnarray*}
\nabla d_\rho(\bx \mid \bx_n) & = & \rho [\bx-P_S(\bx_n)] -\rho [\bx -P_S(\bx)]
 \amp = \amp \rho P_S(\bx) - \rho P_S(\bx_n).
\end{eqnarray*}
Because $P_S(\bx)$ is non-expansive, the gradient is Lipschitz with constant $\rho$. The tangency conditions $d_\rho(\bx_n \mid \bx_n)=0$ and $\nabla d_\rho(\bx_n \mid \bx_n)= {\bf 0}$ therefore yield 
\begin{eqnarray}
d_\rho(\bx \mid \bx_n ) & \le & d_\rho(\bx_n \mid \bx_n) +
 \nabla d_\rho(\bx_n)^t(\bx-\bx_n) +\frac{\rho}{2}\|\bx-\bx_{n}\|^2 \label{error_bd2}\\
& = &\frac{\rho}{2}\|\bx-\bx_{n}\|^2 \nonumber
\end{eqnarray}
for all $\bx \in C$.

At a minimum $\bz_\rho$ of $h_\rho(\bx)$, combining inequalities (\ref{error_bd1}) and (\ref{error_bd2}) gives
\begin{eqnarray*}
h_\rho(\bx_{n+1})+\frac{\rho}{2}\|\bD(\bz_\rho-\bx_{n+1})\|^2 & \le &
g_\rho(\bx_{n+1} \mid \bx_n)+\frac{\rho}{2}
\|\bD(\bz_\rho-\bx_{n+1})\|^2 \\
& \le & g_\rho(\bz_\rho \mid \bx_n) \\
& = & h_\rho(\bz_\rho)+d_\rho(\bD\bz_\rho \mid \bD\bx_n) \\
& \le & h_\rho(\bz_\rho)+\frac{\rho}{2}\|\bD(\bz_\rho-\bx_{n})\|^2 .
\end{eqnarray*}
Adding the result 
\begin{eqnarray*}
h_\rho(\bx_{n+1})-h_\rho(\bz_\rho) & \le &
\frac{\rho}{2}\Big(\|\bD(\bz_\rho-\bx_{n})\|^2
 -\|\bD(\bz_\rho-\bx_{n+1})\|^2\Big)
\end{eqnarray*}
over $n$ and invoking the descent property $h_\rho(\bx_{n+1}) \le h_\rho(\bx_n)$ produce the desired error bound
\begin{eqnarray*}
h_\rho(\bx_{n+1})-h_\rho(\bz_\rho) & \le &
\frac{\rho}{2(n+1)} \Big(\|\bD(\bz_\rho-\bx_{0})\|^2 
-\|\bD(\bz_\rho-\bx_{n+1})\|^2\Big) \\
& \le & \frac{\rho}{2(n+1)}\|\bD(\bz_\rho-\bx_{0})\|^2.
\end{eqnarray*}
\end{proof}
%\qed
%$\bC=\begin{pmatrix}\bA \\ \bB \end{pmatrix}$ 
%\begin{eqnarray*}
%f(\bx) & = & \|\by-\bC\bx\|^2 \\
%\bx & = & (\bC^*\bC)^{-1}\bC^*\by \\
%\bC^*\bC & = & \bA^*\bA+\bB^*\bB \\
%\bC^*\by & = & (\bA^*\,\bB^*)\by \\
%\bx & = & (\bA^*\bA+\bB^*\bB)^{-1}(\bA^* \,\bB^*)\by
%\end{eqnarray*}

\section*{\center Another Algorithm}

Consider minimizing the sum $f(\bx)=\sum_{i=1}^p f_i(\bA_i\bx)$, 
where $f_i(\by_i)$ is $L_i$ smooth. Let $\bA=\begin{pmatrix}\bA_1 \\ \vdots \\ \bA_p \end{pmatrix}$. Projecting $\bu = \begin{pmatrix} \bu_1 \\ \vdots \\ \bu_p \end{pmatrix}$ onto the space spanned by the vectors 
$\begin{pmatrix}\bA_1\bx \\ \vdots \\ \bA_p\bx \end{pmatrix}$
amounts to minimizing $\|\bu-\bA\bx\|^2$. The least squares solution is
\begin{eqnarray*}
\bx & = & (\bA^t\bA)^{-1}\bA^t\bu 
\amp = \amp \Big(\sum_{i=1}^p \bA_i^t \bA_i \Big)^{-1}
\sum_{i=1}^p \bA_i^t \bu_i.
\end{eqnarray*}
Adding the majorizations
\begin{eqnarray*}
f_i(\by_i) & \le & f(\by_{ni})+df_i(\by_{ni})(\by_i-\by_{ni})
+\frac{L_i}{2}\|\by_i-\by_{ni}\|^2 \\
& = & f(\by_{ni}) +\frac{L_i}{2}\Big\|\by_i-\by_{ni}+\frac{1}{L_i}\nabla f_i(\by_{ni})\Big\|^2 
+c_{ni} \\
& = & f(\by_{ni}) +\frac{L_i}{2}\Big\|\by_i-\bu_{ni}\Big\|^2 
+c_{ni}.
\end{eqnarray*}
gives a majorization that we can minimize by projection.
If we have a better quadratic upper bound majorization
\begin{eqnarray*}
f_i(\by_i) & \le & f_i(\by_{ni})+df_i(\by_{ni})(\by_i-\by_{ni})
+\frac{1}{2}(\by_i-\by_{ni})^t\bB_i(\by-\by_{ni}),
\end{eqnarray*}
then we can reparameterize by setting $\bv_i = \bB_i^{1/2}\by_i
=\bB_i^{1/2}\bA_i \bx$. It follows that $g_i(\bv_i)= f_i(\by_i)$ satisfies
\begin{eqnarray*}
g_i(\bv_i) & \le & g_i(\bv_{ni})+dg_i(\bv_{ni})(\bv_i-\bv_{ni})
+\frac{1}{2}\|\bv_i-\bv_{ni}\|^2 \\
& = & g_i(\bv_{ni})+\Big\|\bv_i-\bv_{ni}+\nabla g_i(\bv_i)\Big\|^2 + c_{ni}.
\end{eqnarray*}
and we can proceed as before with $\bA_i$ replaced by $\bB_i^{1/2}\bA_i$. In practice $\bB_i^{1/2}$ should be taken to be the Cholesky decomposition of $\bB_i$. 

\section*{\center Yet Another Algorithm}

Consider minimizing the penalized function
$h_\rho(\bx)=f(\bx)+\frac{\rho}{2} g(\bx)^2$, where $g(\bx) = \dist(\bx,C)$ is a distance constraint, $g(\bx)=0$ represents a surface, or c) $g(\bx) \le 0$ represents a sublevel set, in which case we substitute $g(\bx)_+^2$ for $g(\bx)^2$. To accommodate a fused constraint $g(\bD\bx)=0$, we replace $g(\bx)$ by $g(\bD\bx)$ and $\nabla g(\bx)$ by $\bD^t \nabla g(\bD\bx)$ in our subsequent calculations. In any event, the gradient and Hessian of $h_\rho(\bx)$ amount to 
\begin{eqnarray*}
\nabla h_\rho(\bx) & = & \nabla f(\bx) 
+\rho g(\bx) \nabla g(\bx) \\
d^2h_\rho(\bx) & = & d^2 f(\bx) +\rho \nabla g(\bx)dg(\bx)
+ \rho g(\bx) d^2g(\bx) .
\end{eqnarray*}
When $\rho$ is large and we are near the optimal point, we expect the value $g(\bx)$ to be close to zero. Hence, we approximate
\begin{eqnarray*}
d^2h_\rho(\bx) & \approx & d^2 f(\bx) +\rho \nabla g(\bx)dg(\bx) .
\end{eqnarray*}
If we put $\bH_n=d^2f(\bx_n)$ and $\bv_n = \nabla g(\bx_n)$, then
an approximate Newton step can be written as  
\begin{eqnarray*}
\bx_{n+1} & = & \bx_n - (\bH_n+\rho \bv_n\bv_n^t)^{-1}
\nabla h_\rho(\bx_n) .
\end{eqnarray*}
An application of the Sherman-Morrison formula reduces this update to
\begin{eqnarray}
\bx_{n+1} & = & \bx_n - \bH_n^{-1}\nabla h_\rho(\bx_n)
+ \frac{\rho\bv_n^t \bH_n^{-1}\nabla h_\rho(\bx_n)  }
{1+\rho \bv_n^t\bH_n^{-1} \bv_n}\bH_n^{-1}\bv_n. \label{newtonlike}
\end{eqnarray}
This algorithm possesses several virtues: a) it requires no
second derivatives of $g(\bx)$, which may not exist, b) it can exploit an explicit inverse of $\bH_n$ when the inverse exists, c) it generalizes to multiple constraints, and d) it remains viable when $\bH_n$ simply approximates $d^2f(\bx_n)$. For instance, taking $\bH_n$ equal an expected information matrix is often reasonable.

Set projection falls within this paradigm. In this case
$f(\bx)=\frac{1}{2}\|\by-\bx\|^2$, $\nabla f(\bx)=\bx-\by$, and $\bH_n = \bI$. Any of the three suggested forms of $g(\bx)$ apply in
this context. Formula (\ref{newtonlike}) now reads
\begin{eqnarray*}
\bx_{n+1} & = & \by - \rho g(\bx_n)\bv_n
+ \frac{\rho\bv_n^t [\bx_n-\by+\rho g(\bx_n)\bv_n]  }
{1+\rho \|\bv_n\|^2}\bv_n \\
& = & \by+\rho\frac{\bv_n^t(\bx_n-\by)-g(\bx_n)}
{1+\rho \|\bv_n\|^2}\bv_n
\end{eqnarray*}
with $\bv_n = \nabla g(\bx_n)$. At a stationary point
$\bx -\by + \rho g(\bx) \bv = {\bf 0}$, we have the check
\begin{eqnarray*}
\rho\frac{\bv^t(\bx-\by)-g(\bx)}
{1+\rho \|\bv\|^2}\bv & \!\! = \!\! & \rho\frac{-g(\bx)\rho \|\bv\|^2-g(\bx)}
{1+\rho \|\bv\|^2}\bv_n \; = \; -\rho g(\bx)
\; = \; \bx-\by.
\end{eqnarray*}

 

\begin{thebibliography}{99}
\bibitem{bauschke2017convex}
Bauschke HH, Combettes PL (2017) {\it Convex Analysis and Monotone Operator Theory in Hilbert Spaces, 2nd edition}. Springer
\bibitem{beltrami1970algorithmic}
Beltrami EJ (1970) {\it An Algorithmic Approach to Nonlinear Analysis and Optimization}. Academic Press
\bibitem{courant1943variational}
Courant R (1943) Variational methods for the solution of problems of equilibrium and vibrations. {\it Bulletin of the American Mathematical Society} 49:1--23
\bibitem{hong2016convergence}
Hong M, Luo Z-Q, Razaviyayn M (2016) Convergence analysis of alternating direction method of multipliers for a family of 
nonconvex problems. {\it SIAM Journal on Optimization} 26:337--364
\bibitem{keys19}
Keys KL, Zhou H, Lange K (2019) Proximal distance algorithms: 
theory and practice. {\it Journal Machine Learning Research}
20(66):1--38
\bibitem{lange2016mm}
Lange K (2016) {\it MM Optimization Algorithms}. SIAM
\bibitem{nesterov04}
Nesterov Y (2004) {\it Introductory Lectures on Convex Optimization:  A Basic Course}. Kluwer Academic Publishers
\end{thebibliography}

\end{document}
