\documentclass{article}

\input{preamble.tex}

\title{Example 2: Convex Regression}
\author{}
\date{}

\begin{document}
\maketitle

\section*{\center Model}

Given a set of observations $n$ responses \(y_{i}\) with predictors $\bx_{i} \in \Real^{d}$, our goal is determine the least squares estimator (LSE) \(\hat{\varphi}_{n}\) by solving
\begin{equation*}
    \hat{\varphi}_{n} \in \underset{\psi}{\arg \min}
    \sum_{i=1}^{n} (y_{i} - \psi(\bx_{i}))^{2}.
\end{equation*}
The equivalent finite dimensional problem
\begin{equation*}
    \underset{\btheta; \bXi}{\mathrm{minimize}}
    ~\frac{1}{2} \|\by - \btheta\|_{2}^{2}
    \quad
    \text{subject to}
    ~\theta_{j} + \langle{\bx_{i} - \bx_{j},~\bxi_{j}}\rangle \le \theta_{i}
\end{equation*}
estimates function values \(\theta_{i} = \varphi_{n}(\bx_{i})\) and subgradients \(\bxi_{i} \in \Real^{d}\) at each data point.
The proximal distance version works with the objective
\begin{equation*}
    h_{\rho}(\bx)
    =
    \frac{1}{2} \|\bA \bx - \by\|^{2}_{2}
    + \frac{\rho}{2} \dist(\bD \bx, \Real_{-}^{m})^{2},
\end{equation*}
where $\bx = [\btheta; \vec(\bXi)]$ stacks each optimization variable into a vector of length $n(1+d)$.
This maneuver introduces matrices
\begin{equation*}
    \bA = \begin{bmatrix}
        \bI_{n \times n} & \boldsymbol{0}_{n \times nd}
    \end{bmatrix}
    \qquad
    \bD = \begin{bmatrix}
        \bD_{1} & \bD_{2}
    \end{bmatrix}
\end{equation*}
where $[\bD_{1} \btheta]_{k} = \theta_{j} - \theta_{i}$ and $[\bD_{2} \vec(\Xi)]_{k} = \langle{\bx_{i} - \bx_{j}, \bxi_{j}}\rangle$ according to the ordering $i > j$.

\section*{\center Algorithm Maps}

\subsection*{MM}
Rewrite the surrogate explicitly a least squares problem minimizing $\|\tilde{\bA} \bx - \tilde{\bb}_{n}\|^{2}_{2}$:
\begin{equation*}
  \bx_{n+1} = \underset{\bx}{\argmin} \frac{1}{2} \left\|
    \begin{bmatrix}
      \bA \\
      \sqrt{\rho} \bD
    \end{bmatrix} \bx
    -
    \begin{bmatrix}
      \bb \\
      \sqrt{\rho} \mathcal{P}(\bD \bx_{n})
    \end{bmatrix}
  \right\|_{2}^{2}
\end{equation*}
where $\bb \equiv \by$ to avoid clashing with notation in ADMM below.
In this case it is probably best to store $\bD$ explicitly in order to avoid computing $\bx_{i} - \bx_{j}$ each time one applies $\bD$, $\bD^{t}$, or $\bD^{t} \bD$.

\subsection*{Steepest Descent}

The updates $\bx_{n+1} = \bx_{n} - \gamma_{n} \nabla h_{\rho}(\bx_{n})$ admit an exact solution for the line search parameter $\gamma_{n}$.
Taking $\bq_{n} = \nabla h_{\rho}(\bx_{n})$ as the gradient we have
\begin{align*}
  \bq_{n}
  &= \bA^{t} \bA(\bx_{n} - \bb) + \rho \bD^{t} [\bD \bx_{n} - \mathcal{P}(\bD \bx_{n})] \\
  \gamma_{n}
  &=
  \frac{\|\bq_{n}\|^{2}}{\|\bA \bq_{n}\|^{2} + \rho \|\bD \bq_{n}\|^{2}}.
\end{align*}
Note that $\bA \bq_{n} = \nabla_{\btheta} h_{\rho}(\bx_{n})$, the gradient with respect to function values $\btheta$.

\subsection*{ADMM}

Take $\by$ as the dual variable and $\blambda$ as scaled multipliers.
Minimizing the $\bx$ block updates $(\btheta,\bXi)$ jointly by solving a single linear system.
\begin{align*}
    \bx_{n+1}
    &=
    \underset{\bx}{\argmin} \frac{1}{2} \left\|
        \begin{bmatrix}
        \bA \\
        \sqrt{\mu} \bD
        \end{bmatrix} \bx
        -
        \begin{bmatrix}
        \bb \\
        \sqrt{\mu} (\by_{n} - \blambda_{n})
        \end{bmatrix}
    \right\|_{2}^{2} \\
    \by_{n+1}
    &= \frac{\alpha}{1+\alpha} \mathcal{P}(\bz_{n}) + \frac{1}{1+\alpha} \bz_{n};
    \qquad \bz_{n} = \bD \bx_{n+1} + \blambda_{n},~\alpha = \rho / \mu
    \end{align*}
Multipliers follow the standard update.

%%%%% BIBLIOGRAPHY %%%%%
\begin{thebibliography}{99}
    \bibitem{seijo2011}
    Seijo E, Sen B (2011) {Nonparametric least squares estimation of a multivariate convex regression function}. {\it The Annals of Statistics}. 1633-1657

    \bibitem{mazumder2018}
    Mazumder R, Choudhury A, Iyengar G, Sen B (2018) {A computational framework for multivariate convex regression and its variants}. {\it Journal of the American Statistical Association}. 318-331
\end{thebibliography}
\end{document}