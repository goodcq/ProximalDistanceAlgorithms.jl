\documentclass[11pt]{article}

\input{preamble.tex}

\title{Example 3: Convex Clustering}
\author{}
\date{}

\begin{document}
\maketitle

Convex clustering of \(n\) samples based on \(d\) features can be formulated in terms of the regularized objective
\begin{equation}
    \label{eq:regularized-objective}
    F_{\gamma}(\bX)
    =
    \frac{1}{2} \|\bX - \bU\|_{F}^{2}
    +
    \gamma \sum_{i > j} w_{ij} \|\bX (\be_{i} - \be_{j})\|,
\end{equation}
where \(\bU \in \Real^{d \times n}\) encodes the input data, columns of \(\bX \in \Real^{d \times n}\) represent cluster assignments, and \(\be_{k} \in \Real^{n}\) is a standard basis vector.
Interpreting $(w_{ij})$ as an adjacency matrix, we assume the graph is connected otherwise the objective splits over connected components.
Letting \(S_{\nu}\) denote the set of \(\nu\) block sparse vectors, we pass to the proximal distance framework by considering the penalized objective
\begin{align*}
    h_{\rho}(\bX)
    &=
    \frac{1}{2}\|\bX - \bU\|_{F}^{2}
    +
    \frac{\rho}{2} \dist(\bD \bX, S_{\nu})^{2},
\end{align*}
where matrix $\bD$ acts as a forward difference operator on columns.
Taking $\bx = \vec(\bX)$ and mildly abusing notation, its surrogate is
\begin{equation*}
    g_{\rho}(\bx \mid \bx_{n})
    =
    \frac{1}{2}\|\bx - \bu\|_{2}^{2}
    +
    \frac{\rho}{2} \|\bD \bx - \mathcal{P}_{\nu}(\bD \bx_{n})\|^{2}.
\end{equation*}

\section*{\center Blockwise Sparse Projection}

The projection $\mathcal{P}_{\nu}$ maps a matrix to a sparse representation with $\nu$ non-zero columns (or blocks in the case of the vectorized version).
In the context of clustering, sparsity permits a maximum of $\nu$ violations in consensus constraints $\bx_{i} = \bx_{j}$.
Letting $\Delta_{ij} = \|\bx_{i} - \bx_{j}\|$ denote pairwise distances, we define the projection along blocks $\bv_{k}$ as
\begin{align*}
    \mathcal{P}_{\nu}(\bv_{k})
    =
    \begin{cases}
        \bv_{k}, & \text{if}~\Delta_{k} \in \{\Delta_{(m)}, \Delta_{(m-1)}, \ldots \Delta_{(m-\nu+1)}\} \\
        \boldsymbol{0}, & \text{otherwise}.
    \end{cases}
\end{align*}
Concretely, the magnitude of a difference $\bv_{k}$ must be within the top $\nu$ distances.
An alternative, helpful definition is based on the smallest distances
\begin{align*}
    \mathcal{P}_{\nu}(\bv_{k})
    =
    \begin{cases}
        \boldsymbol{0}, & \text{if}~\Delta_{k} \in \{\Delta_{(1)}, \Delta_{(m-1)}, \ldots \Delta_{(\nu)}\} \\
        \bv_{k}, & \text{otherwise}
    \end{cases}
\end{align*}
Thus, it is enough to find a pivot $\Delta_{(m-\nu+1)}$ or $\Delta_{(\nu)}$.
Because the sparsity parameter $\nu$ has a finite range in $\{0,1,2,\ldots,\binom{n}{2}\}$ one can exploit symmetry to reduce the best/average computational complexity in a search procedure.
Internally, this projection is implemented using a partial sorting algorithm based on quicksort.
Note that this projetion operator is set-valued in general.

\section*{\center Algorithm Maps}

\subsection*{MM}
Rewrite the surrogate explicitly a least squares problem minimizing $\|\bA \bx - \bb_{n}\|^{2}_{2}$:
\begin{equation*}
  \bx_{n+1} = \underset{\bx}{\argmin} \frac{1}{2} \left\|
    \begin{bmatrix}
      \bI \\
      \sqrt{\rho} \bD
    \end{bmatrix} \bx
    -
    \begin{bmatrix}
      \bu \\
      \sqrt{\rho} \mathcal{P}_{\nu}(\bD \bx_{n})
    \end{bmatrix}
  \right\|_{2}^{2}
\end{equation*}

\subsection*{Steepest Descent}

The updates $\bx_{n+1} = \bx_{n} - \gamma_{n} \nabla h_{\rho}(\bx_{n})$ admit an exact solution for the line search parameter $\gamma_{n}$.
Taking $\bq_{n} = \nabla h_{\rho}(\bx_{n})$ as the gradient we have
\begin{align*}
  \bq_{n}
  &= (\bx_{n} - \bu) + \rho \bD^{t} [\bD \bx_{n} - \mathcal{P}_{\nu}(\bD \bx_{n})] \\
  \gamma_{n}
  &=
  \frac{\|\bq_{n}\|^{2}}{\|\bq_{n}\|^{2} + \rho \|\bD \bq_{n}\|^{2}}.
\end{align*}
Note that blocks in $[\bD \bx_{n} - \mathcal{P}_{\nu}(\bD\bx_{n})]_{k}$ are equal to $\boldsymbol{0}$ whenever the projection of $[\bD \bx_{n}]_{k}$ is non-zero.

\subsection*{ADMM}

Take $\by$ as the dual variable and $\blambda$ as scaled multipliers.
Minimizing the $\bx$ block involves solving a single linear system:
\begin{align*}
    \bx_{n+1}
    &=
    \underset{\bx}{\argmin} \frac{1}{2} \left\|
        \begin{bmatrix}
        \bI \\
        \sqrt{\mu} \bD
        \end{bmatrix} \bx
        -
        \begin{bmatrix}
        \bu \\
        \sqrt{\mu} (\blambda_{n} - \by_{n})
        \end{bmatrix}
    \right\|_{2}^{2} \\
    \by_{n+1}
    &= \frac{\alpha}{1+\alpha} \mathcal{P}_{\nu}(\bz_{n}) + \frac{1}{1+\alpha} \bz_{n};
    \qquad \bz_{n} = \bD \bx_{n+1} + \blambda_{n},~\alpha = \rho / \mu
    \end{align*}
Multipliers follow the standard update.

\begin{thebibliography}{1}
    \bibitem{chi2015}
    Chi, E. C., Lange, K. (2015). {Splitting Methods for Convex Clustering}. {Journal of Computational and Graphical Statistics}, 24(4), 994â€“1013.
\end{thebibliography}
\end{document}